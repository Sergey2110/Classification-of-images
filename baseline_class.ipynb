{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Gzuv2run9Yxa"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import gc\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "#import numpy as\n",
        "import os\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from IPython.display import clear_output\n",
        "from PIL import Image\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets, models, transforms\n",
        "#from torchvision.models import resnet18\n",
        "from tqdm import tqdm\n",
        "import warnings"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DIR_TRAIN = \"/content/Classification-of-construction-equipment-objects/train/\"\n",
        "DIR_TEST = \"/content/Classification-of-construction-equipment-objects/test/\"\n",
        "\n",
        "PATH_TRAIN = DIR_TRAIN + \"train.csv\"\n",
        "PATH_TEST = DIR_TEST + \"test.csv\""
      ],
      "metadata": {
        "id": "JtcHBYJ9oxVx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://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@github.com/Sergey2110/Classification-of-construction-equipment-objects.git\n",
        "%cd Classification-of-construction-equipment-objects"
      ],
      "metadata": {
        "id": "puFPw94J3f_o",
        "outputId": "312f2abf-afbf-45a3-fa15-072ffe90a036",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Classification-of-construction-equipment-objects'...\n",
            "remote: Enumerating objects: 7188, done.\u001b[K\n",
            "remote: Counting objects: 100% (7188/7188), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7174/7174), done.\u001b[K\n",
            "remote: Total 7188 (delta 19), reused 7172 (delta 10), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (7188/7188), 27.01 MiB | 36.63 MiB/s, done.\n",
            "Resolving deltas: 100% (19/19), done.\n",
            "/content/Classification-of-construction-equipment-objects\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "warnings.simplefilter('ignore')"
      ],
      "metadata": {
        "id": "8LotOtK-sFsV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, data_df, transform=None):\n",
        "        self.data_df = data_df\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # достаем имя изображения и ее лейбл\n",
        "        image_name, label = self.data_df.iloc[idx]['ID_img'], self.data_df.iloc[idx]['class']\n",
        "\n",
        "        # читаем картинку. read the image\n",
        "        image = cv2.imread(DIR_TRAIN + f\"{image_name}\")\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        image = Image.fromarray(image)\n",
        "\n",
        "        # преобразуем, если нужно. transform it, if necessary\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, torch.tensor(label).long()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_df)"
      ],
      "metadata": {
        "id": "h72XzlyfMZek"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TestImageDataset(Dataset):\n",
        "    def __init__(self, data_df, transform=None):\n",
        "        self.data_df = data_df\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_name = self.data_df.iloc[idx]['ID_img']\n",
        "\n",
        "        # читаем картинку\n",
        "        image = cv2.imread(DIR_TEST + f\"{image_name}\")\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        image = Image.fromarray(image)\n",
        "\n",
        "        # преобразуем, если нужно\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_df)"
      ],
      "metadata": {
        "id": "7URcg7KlMqbV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Classification:\n",
        "    def __init__(self):\n",
        "        print(\"Обучающей выборки \", len(os.listdir(DIR_TRAIN)))\n",
        "        print(\"Тестовой выборки \", len(os.listdir(DIR_TEST)))\n",
        "\n",
        "        gc.collect()\n",
        "        # задаем преобразование изображения.\n",
        "        self.train_transform = transforms.Compose([\n",
        "            # transforms.Resize(256),\n",
        "            transforms.RandomResizedCrop(256),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225]),\n",
        "        ])\n",
        "\n",
        "        self.valid_transform = transforms.Compose([\n",
        "            # transforms.Resize(256),\n",
        "            transforms.RandomResizedCrop(256),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225]),\n",
        "        ])\n",
        "\n",
        "        self.data_df = pd.read_csv(PATH_TRAIN)\n",
        "        # self.data_df.head(3)\n",
        "\n",
        "        # разделим датасет на трейн и валидацию, чтобы смотреть на качество\n",
        "        self.train_df, self.valid_df = train_test_split(self.data_df, test_size=0.2, random_state=43)\n",
        "        train_dataset = ImageDataset(self.train_df, self.train_transform)\n",
        "        valid_dataset = ImageDataset(self.valid_df, self.valid_transform)\n",
        "\n",
        "        self.train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                                        batch_size=2,\n",
        "                                                        shuffle=True,\n",
        "                                                        pin_memory=True,\n",
        "                                                        num_workers=2)\n",
        "\n",
        "        self.valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset,\n",
        "                                                        batch_size=1,\n",
        "                                                        # shuffle=True,\n",
        "                                                        pin_memory=True,\n",
        "                                                        num_workers=2)\n",
        "\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        self.dict_acc_for_batch = {\"train\": {}, \"test\": {}}\n",
        "        self.dict_loss_for_batch = {\"train\": {}, \"test\": {}}\n",
        "\n",
        "    def crossvalid(self, res_model=None, criterion=None, optimizer=None, dataset=None, k_fold=5):\n",
        "        train_score = pd.Series()\n",
        "        val_score = pd.Series()\n",
        "\n",
        "        total_size = len(dataset)\n",
        "        fraction = 1 / k_fold\n",
        "        seg = int(total_size * fraction)\n",
        "        # tr:train,val:valid; r:right,l:left;  eg: trrr: right index of right side train subset\n",
        "        # index: [trll,trlr],[vall,valr],[trrl,trrr]\n",
        "        for i in range(k_fold):\n",
        "            trll = 0\n",
        "            trlr = i * seg\n",
        "            vall = trlr\n",
        "            valr = i * seg + seg\n",
        "            trrl = valr\n",
        "            trrr = total_size\n",
        "\n",
        "            train_left_indices = list(range(trll, trlr))\n",
        "            train_right_indices = list(range(trrl, trrr))\n",
        "\n",
        "            train_indices = train_left_indices + train_right_indices\n",
        "            val_indices = list(range(vall, valr))\n",
        "\n",
        "            train_set = torch.utils.data.dataset.Subset(dataset, train_indices)\n",
        "            val_set = torch.utils.data.dataset.Subset(dataset, val_indices)\n",
        "\n",
        "            train_loader = torch.utils.data.DataLoader(train_set, batch_size=50,\n",
        "                                                       shuffle=True, num_workers=4)\n",
        "            val_loader = torch.utils.data.DataLoader(val_set, batch_size=50,\n",
        "                                                     shuffle=True, num_workers=4)\n",
        "            train_acc = self.train(res_model, criterion, optimizer, train_loader, val_loader, 1)\n",
        "            train_score.at[i] = train_acc\n",
        "            # val_acc = valid(res_model, criterion, optimizer, val_loader)\n",
        "            # val_score.at[i] = val_acc\n",
        "\n",
        "        return train_score, val_score\n",
        "\n",
        "    def plot_history(self, train_history, val_history, title = 'loss'):\n",
        "        plt.figure()\n",
        "        plt.title('{}'.format(title))\n",
        "        print(train_history)\n",
        "        dd = list(map(lambda x: x.cpu().detach().numpy(), train_history))\n",
        "        plt.plot(dd, label='train', zorder=1)\n",
        "\n",
        "        # points = np.array(val_history)\n",
        "        steps = list(range(0, len(train_history) + 1, int(len(train_history) / len(val_history))))[1:]\n",
        "\n",
        "        plt.scatter(steps, val_history, marker='+', s=180, c='orange', label='val', zorder=2)\n",
        "        plt.xlabel('train steps')\n",
        "\n",
        "        plt.legend(loc='best')\n",
        "        plt.grid()\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "    def train(self, criterion, optimizer, train_dataloader, test_dataloader, NUM_EPOCH=15, show_img=False):\n",
        "        train_loss_log = []\n",
        "        val_loss_log = []\n",
        "\n",
        "        train_acc_log = []\n",
        "        val_acc_log = []\n",
        "\n",
        "        for epoch in tqdm(range(15)):\n",
        "            self.model.train()\n",
        "            train_loss = 0.\n",
        "            train_size = 0\n",
        "            train_pred = 0.\n",
        "\n",
        "            print(\"train\")\n",
        "            for imgs, labels in train_dataloader:\n",
        "                optimizer.zero_grad()\n",
        "                # print(labels)\n",
        "\n",
        "                imgs = imgs.to(self.device)\n",
        "                labels = labels.to(self.device)\n",
        "\n",
        "                y_pred = self.model(imgs)\n",
        "\n",
        "                loss = criterion(y_pred, labels)\n",
        "                loss.backward()\n",
        "\n",
        "                train_loss += loss.item()\n",
        "                train_size += y_pred.size(0)\n",
        "                train_loss_log.append(loss.data / y_pred.size(0))\n",
        "                train_pred += (y_pred.argmax(1) == labels).sum()\n",
        "                optimizer.step()\n",
        "\n",
        "            train_loss_log.append(train_loss / train_size)\n",
        "            train_acc_log.append(train_pred / train_size)\n",
        "\n",
        "            self.dict_loss_for_batch[\"train\"].update({epoch: train_loss_log[:]})\n",
        "            self.dict_acc_for_batch[\"train\"].update({epoch: train_acc_log[:]})\n",
        "\n",
        "            # if show_img and epoch > (epoch - 2) and train_pred / train_size < 0.9:\n",
        "            #     for j in range(4):\n",
        "            #         show_input(imgs[j].cpu(),\n",
        "            #                    title=f\"{labels[j]} {list_file[list_index_val[j + i * batch_size_v]][0]}\")\n",
        "            #         print(f\" epoch = {epoch} acc = {(train_pred / train_size) / batch_size_v}\")\n",
        "\n",
        "            val_loss = 0.\n",
        "            val_size = 0\n",
        "            val_pred = 0.\n",
        "            self.model.eval()\n",
        "\n",
        "            print(\"test\")\n",
        "            with torch.no_grad():\n",
        "                for imgs, labels in test_dataloader:\n",
        "                    imgs = imgs.to(self.device)\n",
        "                    labels = labels.to(self.device)\n",
        "                    print(labels)\n",
        "\n",
        "                    pred = self.model(imgs)\n",
        "                    loss = criterion(pred, labels)\n",
        "\n",
        "                    val_loss += loss.item()\n",
        "                    val_size += pred.size(0)\n",
        "                    val_pred += (pred.argmax(1) == labels).sum()\n",
        "\n",
        "            val_loss_log.append(val_loss / val_size)\n",
        "            val_acc_log.append(val_pred / val_size)\n",
        "\n",
        "            self.dict_loss_for_batch[\"test\"].update({epoch: val_loss_log[:]})\n",
        "            self.dict_acc_for_batch[\"test\"].update({epoch: val_acc_log[:]})\n",
        "\n",
        "            clear_output()\n",
        "            self.plot_history(train_loss_log, val_loss_log, 'loss')\n",
        "\n",
        "            print('Train loss:', (train_loss / train_size) * 100)\n",
        "            print('Val loss:', (val_loss / val_size) * 100)\n",
        "            print('Train acc:', (train_pred / train_size) * 100)\n",
        "            print('Val acc:', (val_pred / val_size) * 100)\n",
        "\n",
        "        return train_loss_log, train_acc_log, val_loss_log, val_acc_log\n",
        "\n",
        "    def watch_img(self):\n",
        "        # посмотрим на картинки. Не забудем указать корретный путь до папки\n",
        "        sns.countplot(x=\"class\", data=self.data_df)\n",
        "        fig, axs = plt.subplots(2, 4, figsize=(16, 8))\n",
        "        fig.suptitle(f'Автомобиль {\" \" * 105} Кран', fontsize=14)\n",
        "\n",
        "        for i, name in zip(range(4), self.data_df[self.data_df['class'] == 1].sample(4, random_state=42)['ID_img']):\n",
        "            img = plt.imread(DIR_TRAIN + f\"{name}\")\n",
        "            axs[i // 2, (i % 2)].imshow(img)\n",
        "            axs[i // 2, (i % 2)].axis('off')\n",
        "\n",
        "        for i, name in zip(range(4), self.data_df[self.data_df['class'] == 0].sample(4, random_state=42)['ID_img']):\n",
        "            img = plt.imread(DIR_TRAIN + f\"{name}\")\n",
        "            axs[i // 2, (i % 2) + 2].imshow(img)\n",
        "            axs[i // 2, (i % 2) + 2].axis('off')\n",
        "\n",
        "        fig.tight_layout()\n",
        "        fig.subplots_adjust(top=0.88)\n",
        "\n",
        "    def train_model(self):\n",
        "        self.model = models.resnet152(pretrained=True)\n",
        "        self.model.fc = nn.Linear(2048, 8)\n",
        "        self.model = self.model.to(self.device)\n",
        "        criterion = torch.nn.CrossEntropyLoss()\n",
        "        optimizer = torch.optim.Adam(self.model.fc.parameters(), lr=0.01)\n",
        "        train_loss_log, train_acc_log, val_loss_log, val_acc_log = self.train(criterion,\n",
        "                                                                              optimizer,\n",
        "                                                                              self.train_loader,\n",
        "                                                                              self.valid_loader,\n",
        "                                                                              5)\n",
        "        return train_loss_log, train_acc_log, val_loss_log, val_acc_log\n",
        "\n",
        "    def evaluation_model(self):\n",
        "        valid_predicts = []\n",
        "        self.model.eval()\n",
        "        for imgs, _ in tqdm(self.valid_loader):\n",
        "            imgs = imgs.to(self.device)\n",
        "            pred = self.model(imgs)\n",
        "            pred_numpy = pred.cpu().detach().numpy()\n",
        "            for class_obj in pred_numpy:\n",
        "                index, max_value = max(enumerate(class_obj), key=lambda i_v: i_v[1])\n",
        "                valid_predicts.append(index)\n",
        "        self.valid_df[\"pred\"] = valid_predicts\n",
        "        val_accuracy = recall_score(self.valid_df['class'].values, self.valid_df['pred'].values, average=\"macro\")\n",
        "        print(f\"Validation accuracy = {val_accuracy}\")\n",
        "\n",
        "        self.test_df = pd.read_csv(PATH_TEST)\n",
        "        self.test_df = self.test_df.drop([\"class\"], axis=1)\n",
        "        self.test_dataset = TestImageDataset(self.test_df, self.valid_transform)\n",
        "        self.test_loader = torch.utils.data.DataLoader(dataset=self.test_dataset,\n",
        "                                                       batch_size=32,\n",
        "                                                       # shuffle=True,\n",
        "                                                       pin_memory=True,\n",
        "                                                       num_workers=2)\n",
        "\n",
        "    def create_submit(self):\n",
        "        self.model.eval()\n",
        "        predicts = []\n",
        "        for imgs in tqdm(self.test_loader):\n",
        "            imgs = imgs.to(self.device)\n",
        "            pred = self.model(imgs)\n",
        "            for class_obj in pred:\n",
        "                index, max_value = max(enumerate(class_obj), key=lambda i_v: i_v[1])\n",
        "                predicts.append(index)\n",
        "\n",
        "        self.test_df[\"class\"] = predicts\n",
        "        self.test_df.head()\n",
        "        self.test_df.to_csv(\"submit.csv\", index=False)"
      ],
      "metadata": {
        "id": "WTzOYXnLr615"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classification = Classification()\n",
        "acc_loss = classification.train_model()\n",
        "# classification.evaluation_model()\n",
        "# classification.create_submit()"
      ],
      "metadata": {
        "id": "NtXeREGor-ln",
        "outputId": "924fce5a-f263-4f93-c5c4-e82deaad26c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 815
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/15 [05:35<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-aefa4720c399>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mclassification\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClassification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0macc_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# classification.evaluation_model()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# classification.create_submit()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-74b421a651e5>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         train_loss_log, train_acc_log, val_loss_log, val_acc_log = self.train(criterion,\n\u001b[0m\u001b[1;32m    209\u001b[0m                                                                               \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                                                                               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-74b421a651e5>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, criterion, optimizer, train_dataloader, test_dataloader, NUM_EPOCH, show_img)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss_log\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss_log\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train loss:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-74b421a651e5>\u001b[0m in \u001b[0;36mplot_history\u001b[0;34m(self, train_history, val_history, title)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_history\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzorder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-74b421a651e5>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_history\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzorder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'cpu'"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}