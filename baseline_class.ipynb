{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Gzuv2run9Yxa"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import gc\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import warnings\n",
        "\n",
        "from IPython.display import clear_output\n",
        "from PIL import Image\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets, models, transforms\n",
        "#from torchvision.models import resnet18\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DIR_TRAIN = \"/content/Classification-of-construction-equipment-objects/train/\"\n",
        "DIR_TEST = \"/content/Classification-of-construction-equipment-objects/test/\"\n",
        "\n",
        "PATH_TRAIN = DIR_TRAIN + \"train.csv\"\n",
        "PATH_TEST = DIR_TEST + \"test.csv\""
      ],
      "metadata": {
        "id": "JtcHBYJ9oxVx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://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@github.com/Sergey2110/Classification-of-construction-equipment-objects.git\n",
        "%cd Classification-of-construction-equipment-objects"
      ],
      "metadata": {
        "id": "puFPw94J3f_o",
        "outputId": "d5eecd2c-4a4c-4c22-ccd9-92b702cca29d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Classification-of-construction-equipment-objects'...\n",
            "remote: Enumerating objects: 7242, done.\u001b[K\n",
            "remote: Counting objects: 100% (7242/7242), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7228/7228), done.\u001b[K\n",
            "remote: Total 7242 (delta 50), reused 7172 (delta 10), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (7242/7242), 30.80 MiB | 15.53 MiB/s, done.\n",
            "Resolving deltas: 100% (50/50), done.\n",
            "/content/Classification-of-construction-equipment-objects\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "warnings.simplefilter('ignore')"
      ],
      "metadata": {
        "id": "8LotOtK-sFsV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, data_df, transform=None):\n",
        "        self.data_df = data_df\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # достаем имя изображения и ее лейбл\n",
        "        image_name, label = self.data_df.iloc[idx]['ID_img'], self.data_df.iloc[idx]['class']\n",
        "\n",
        "        # читаем картинку. read the image\n",
        "        image = cv2.imread(DIR_TRAIN + f\"{image_name}\")\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        image = Image.fromarray(image)\n",
        "\n",
        "        # преобразуем, если нужно. transform it, if necessary\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, torch.tensor(label).long()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_df)"
      ],
      "metadata": {
        "id": "h72XzlyfMZek"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TestImageDataset(Dataset):\n",
        "    def __init__(self, data_df, transform=None):\n",
        "        self.data_df = data_df\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_name = self.data_df.iloc[idx]['ID_img']\n",
        "\n",
        "        # читаем картинку\n",
        "        image = cv2.imread(DIR_TEST + f\"{image_name}\")\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        image = Image.fromarray(image)\n",
        "\n",
        "        # преобразуем, если нужно\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_df)"
      ],
      "metadata": {
        "id": "7URcg7KlMqbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Classification:\n",
        "    def __init__(self):\n",
        "        print(\"Обучающей выборки \", len(os.listdir(DIR_TRAIN)))\n",
        "        print(\"Тестовой выборки \", len(os.listdir(DIR_TEST)))\n",
        "\n",
        "        gc.collect()\n",
        "        torch.manual_seed(0)\n",
        "        torch.cuda.manual_seed(0)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "\n",
        "        # задаем преобразование изображения.\n",
        "        self.train_transform = transforms.Compose([\n",
        "            # transforms.Resize(256),\n",
        "            transforms.RandomResizedCrop(256),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225]),\n",
        "        ])\n",
        "\n",
        "        self.valid_transform = transforms.Compose([\n",
        "            # transforms.Resize(256),\n",
        "            transforms.RandomResizedCrop(256),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225]),\n",
        "        ])\n",
        "\n",
        "        self.data_df = pd.read_csv(PATH_TRAIN)\n",
        "        self.data_df = self.data_df[:2000]\n",
        "        # self.data_df.head(3)\n",
        "\n",
        "        # разделим датасет на трейн и валидацию, чтобы смотреть на качество\n",
        "        self.train_df, self.valid_df = train_test_split(self.data_df, test_size=0.2)\n",
        "        train_dataset = ImageDataset(self.train_df, self.train_transform)\n",
        "        valid_dataset = ImageDataset(self.valid_df, self.valid_transform)\n",
        "\n",
        "        self.train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                                        batch_size=5,\n",
        "                                                        shuffle=True,\n",
        "                                                        pin_memory=True,\n",
        "                                                        num_workers=2)\n",
        "\n",
        "        self.valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset,\n",
        "                                                        batch_size=5,\n",
        "                                                        shuffle=True,\n",
        "                                                        pin_memory=True,\n",
        "                                                        num_workers=2)\n",
        "\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        self.dict_acc_for_batch = {\"train\": {}, \"test\": {}}\n",
        "        self.dict_loss_for_batch = {\"train\": {}, \"test\": {}}\n",
        "\n",
        "    def crossvalid(self, res_model=None, criterion=None, optimizer=None, dataset=None, k_fold=5):\n",
        "        train_score = pd.Series()\n",
        "        val_score = pd.Series()\n",
        "\n",
        "        total_size = len(dataset)\n",
        "        fraction = 1 / k_fold\n",
        "        seg = int(total_size * fraction)\n",
        "        # tr:train,val:valid; r:right,l:left;  eg: trrr: right index of right side train subset\n",
        "        # index: [trll,trlr],[vall,valr],[trrl,trrr]\n",
        "        for i in range(k_fold):\n",
        "            trll = 0\n",
        "            trlr = i * seg\n",
        "            vall = trlr\n",
        "            valr = i * seg + seg\n",
        "            trrl = valr\n",
        "            trrr = total_size\n",
        "\n",
        "            train_left_indices = list(range(trll, trlr))\n",
        "            train_right_indices = list(range(trrl, trrr))\n",
        "\n",
        "            train_indices = train_left_indices + train_right_indices\n",
        "            val_indices = list(range(vall, valr))\n",
        "\n",
        "            train_set = torch.utils.data.dataset.Subset(dataset, train_indices)\n",
        "            val_set = torch.utils.data.dataset.Subset(dataset, val_indices)\n",
        "\n",
        "            train_loader = torch.utils.data.DataLoader(train_set, batch_size=50,\n",
        "                                                       shuffle=True, num_workers=4)\n",
        "            val_loader = torch.utils.data.DataLoader(val_set, batch_size=50,\n",
        "                                                     shuffle=True, num_workers=4)\n",
        "            train_acc = self.train(res_model, criterion, optimizer, train_loader, val_loader, 1)\n",
        "            train_score.at[i] = train_acc\n",
        "            # val_acc = valid(res_model, criterion, optimizer, val_loader)\n",
        "            # val_score.at[i] = val_acc\n",
        "\n",
        "        return train_score, val_score\n",
        "\n",
        "    def plot_history(self, train_history, val_history, title = 'Ошибка'):\n",
        "        plt.figure()\n",
        "        plt.title('{}'.format(title))\n",
        "\n",
        "        plt.plot(train_history, label='train', zorder=1)\n",
        "        plt.plot(val_history, label='val', zorder=1)\n",
        "\n",
        "        plt.legend(loc='best')\n",
        "        plt.xlabel('steps')\n",
        "        plt.grid()\n",
        "        plt.show()\n",
        "\n",
        "    def train(self, criterion, optimizer, train_dataloader, test_dataloader, NUM_EPOCH=15, show_img=False):\n",
        "        train_loss_log = []\n",
        "        val_loss_log = []\n",
        "        train_acc_log = []\n",
        "        val_acc_log = []\n",
        "\n",
        "        train_loss_log_for_batch = []\n",
        "        val_loss_log_for_batch = []\n",
        "        train_acc_log_for_batch = []\n",
        "        val_acc_log_for_batch = []\n",
        "\n",
        "        for epoch in tqdm(range(NUM_EPOCH)):\n",
        "            self.model.train()\n",
        "            train_loss = 0.\n",
        "            train_size = 0\n",
        "            train_pred = 0.\n",
        "\n",
        "            for imgs, labels in train_dataloader:\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                imgs = imgs.to(self.device)\n",
        "                labels = labels.to(self.device)\n",
        "\n",
        "                y_pred = self.model(imgs)\n",
        "\n",
        "                loss = criterion(y_pred, labels)\n",
        "                loss.backward()\n",
        "\n",
        "                train_size += y_pred.size(0)\n",
        "                train_loss += loss.item()\n",
        "                train_loss_log_for_batch.append(loss.data / y_pred.size(0))\n",
        "                train_pred += (y_pred.argmax(1) == labels).sum()\n",
        "                train_acc_log_for_batch.append(train_pred / y_pred.size(0))\n",
        "                optimizer.step()\n",
        "\n",
        "            train_loss_log.append(train_loss / train_size)\n",
        "            train_acc_log.append(train_pred.item() / train_size)\n",
        "\n",
        "            self.dict_loss_for_batch[\"train\"].update({epoch: train_loss_log_for_batch[:]})\n",
        "            self.dict_acc_for_batch[\"train\"].update({epoch: train_acc_log_for_batch[:]})\n",
        "\n",
        "            # if show_img and epoch > (epoch - 2) and train_pred / train_size < 0.9:\n",
        "            #     for j in range(4):\n",
        "            #         show_input(imgs[j].cpu(),\n",
        "            #                    title=f\"{labels[j]} {list_file[list_index_val[j + i * batch_size_v]][0]}\")\n",
        "            #         print(f\" epoch = {epoch} acc = {(train_pred / train_size) / batch_size_v}\")\n",
        "\n",
        "            val_loss = 0.\n",
        "            val_size = 0\n",
        "            val_pred = 0.\n",
        "            self.model.eval()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for imgs, labels in test_dataloader:\n",
        "                    imgs = imgs.to(self.device)\n",
        "                    labels = labels.to(self.device)\n",
        "\n",
        "                    pred = self.model(imgs)\n",
        "                    loss = criterion(pred, labels)\n",
        "\n",
        "                    val_size += pred.size(0)\n",
        "                    val_loss += loss.item()\n",
        "                    val_loss_log_for_batch.append(loss.data / pred.size(0))\n",
        "                    val_pred += (pred.argmax(1) == labels).sum()\n",
        "                    val_acc_log_for_batch.append(val_pred / pred.size(0))\n",
        "\n",
        "            val_loss_log.append(val_loss / val_size)\n",
        "            val_acc_log.append(val_pred.item() / val_size)\n",
        "\n",
        "            self.dict_loss_for_batch[\"test\"].update({epoch: val_loss_log_for_batch[:]})\n",
        "            self.dict_acc_for_batch[\"test\"].update({epoch: val_acc_log_for_batch[:]})\n",
        "\n",
        "        # clear_output()\n",
        "\n",
        "        print(f'\\nМин. и макс. потери на тренировочных и проверочных данных: {min(train_loss_log)},  {max(train_loss_log)},  {min(val_loss_log)},  {max(val_loss_log)}')\n",
        "        print(f'Мин. и макс. верность на тренировочных и проверочных данных: {min(train_acc_log)},  {max(train_acc_log)},  {min(val_acc_log)},  {max(val_acc_log)}')\n",
        "        print(f'Потери на тренировке и проверке: {(train_loss / train_size) * 100},  {(val_loss / val_size) * 100}')\n",
        "        print(f'Верность на тренировке и проверке: {(train_pred.item() / train_size) * 100},  {(val_pred.item() / val_size) * 100}')\n",
        "\n",
        "        return train_loss_log, train_acc_log, val_loss_log, val_acc_log\n",
        "\n",
        "    def watch_img(self):\n",
        "        # посмотрим на картинки. Не забудем указать корретный путь до папки\n",
        "        sns.countplot(x=\"class\", data=self.data_df)\n",
        "        fig, axs = plt.subplots(2, 4, figsize=(16, 8))\n",
        "        fig.suptitle(f'Автомобиль {\" \" * 105} Кран', fontsize=14)\n",
        "\n",
        "        for i, name in zip(range(4), self.data_df[self.data_df['class'] == 1].sample(4, random_state=42)['ID_img']):\n",
        "            img = plt.imread(DIR_TRAIN + f\"{name}\")\n",
        "            axs[i // 2, (i % 2)].imshow(img)\n",
        "            axs[i // 2, (i % 2)].axis('off')\n",
        "\n",
        "        for i, name in zip(range(4), self.data_df[self.data_df['class'] == 0].sample(4, random_state=42)['ID_img']):\n",
        "            img = plt.imread(DIR_TRAIN + f\"{name}\")\n",
        "            axs[i // 2, (i % 2) + 2].imshow(img)\n",
        "            axs[i // 2, (i % 2) + 2].axis('off')\n",
        "\n",
        "        fig.tight_layout()\n",
        "        fig.subplots_adjust(top=0.88)\n",
        "\n",
        "    def train_model(self, epoch=5):\n",
        "        self.model = models.resnet152(pretrained=True)\n",
        "        self.model.fc = nn.Linear(2048, 8)\n",
        "        # self.model.fc = nn.Softmax(self.model.layer4)\n",
        "        self.model = self.model.to(self.device)\n",
        "        criterion = torch.nn.CrossEntropyLoss()\n",
        "        optimizer = torch.optim.Adam(self.model.fc.parameters(), lr=0.01)\n",
        "        train_loss_log, train_acc_log, val_loss_log, val_acc_log = self.train(criterion,\n",
        "                                                                              optimizer,\n",
        "                                                                              self.train_loader,\n",
        "                                                                              self.valid_loader,\n",
        "                                                                              epoch)\n",
        "        return train_loss_log, train_acc_log, val_loss_log, val_acc_log\n",
        "\n",
        "    def evaluation_model(self):\n",
        "        valid_predicts = []\n",
        "        self.model.eval()\n",
        "        for imgs, _ in tqdm(self.valid_loader):\n",
        "            imgs = imgs.to(self.device)\n",
        "            pred = self.model(imgs)\n",
        "            pred_numpy = pred.cpu().detach().numpy()\n",
        "            for class_obj in pred_numpy:\n",
        "                index, max_value = max(enumerate(class_obj), key=lambda i_v: i_v[1])\n",
        "                valid_predicts.append(index)\n",
        "        self.valid_df[\"pred\"] = valid_predicts\n",
        "        val_accuracy = recall_score(self.valid_df['class'].values, self.valid_df['pred'].values, average=\"macro\")\n",
        "        print(f\"Validation accuracy = {val_accuracy}\")\n",
        "\n",
        "        self.test_df = pd.read_csv(PATH_TEST)\n",
        "        self.test_df = self.test_df.drop([\"class\"], axis=1)\n",
        "        self.test_dataset = TestImageDataset(self.test_df, self.valid_transform)\n",
        "        self.test_loader = torch.utils.data.DataLoader(dataset=self.test_dataset,\n",
        "                                                       batch_size=32,\n",
        "                                                       # shuffle=True,\n",
        "                                                       pin_memory=True,\n",
        "                                                       num_workers=2)\n",
        "\n",
        "    def create_submit(self):\n",
        "        self.model.eval()\n",
        "        predicts = []\n",
        "        for imgs in tqdm(self.test_loader):\n",
        "            imgs = imgs.to(self.device)\n",
        "            pred = self.model(imgs)\n",
        "            for class_obj in pred:\n",
        "                index, max_value = max(enumerate(class_obj), key=lambda i_v: i_v[1])\n",
        "                predicts.append(index)\n",
        "\n",
        "        self.test_df[\"class\"] = predicts\n",
        "        self.test_df.head()\n",
        "        self.test_df.to_csv(\"submit.csv\", index=False)"
      ],
      "metadata": {
        "id": "WTzOYXnLr615"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classification = Classification()\n",
        "# print(list(classification.train_loader))\n",
        "train_loss_log, train_acc_log, val_loss_log, val_acc_log = classification.train_model(epoch=20)\n",
        "classification.plot_history(train_loss_log, val_loss_log)\n",
        "classification.plot_history(train_acc_log, val_acc_log, 'Верность')\n",
        "# classification.evaluation_model()\n",
        "# classification.create_submit()"
      ],
      "metadata": {
        "id": "NtXeREGor-ln",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        },
        "outputId": "5db53d18-7867-4415-eaef-4a978bd3f5be"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Обучающей выборки  4993\n",
            "Тестовой выборки  2141\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/20 [00:08<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-a0cb0ca5dc88>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mclassification\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClassification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# print(list(classification.train_loader))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_loss_log\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc_log\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss_log\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc_log\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mclassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss_log\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss_log\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mclassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_acc_log\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc_log\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Верность'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-77398a75320a>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         train_loss_log, train_acc_log, val_loss_log, val_acc_log = self.train(criterion,\n\u001b[0m\u001b[1;32m    211\u001b[0m                                                                               \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m                                                                               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-77398a75320a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, criterion, optimizer, train_dataloader, test_dataloader, NUM_EPOCH, show_img)\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m                 \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavgpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m   1480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1481\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1482\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1841\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"softmax\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1842\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1843\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1844\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1845\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: softmax() received an invalid combination of arguments - got (Sequential), but expected one of:\n * (int dim, torch.dtype dtype)\n * (name dim, *, torch.dtype dtype)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_weights = []\n",
        "conv_layers = []\n",
        "counter = 0\n",
        "num_layer = 12\n",
        "\n",
        "model_children = list(classification.model.cpu().children())\n",
        "\n",
        "for i in range(len(model_children)):\n",
        "  if type(model_children[i]) == nn.Conv2d:\n",
        "    counter += 1\n",
        "    model_weights.append(model_children[i].weight)\n",
        "    conv_layers.append(model_children[i])\n",
        "  elif type(model_children[i]) == nn.Sequential:\n",
        "    for j in range(len(model_children[i])):\n",
        "      for child in model_children[i][j].children():\n",
        "        if type(child) == nn.Conv2d:\n",
        "          counter += 1\n",
        "          model_weights.append(child.weight)\n",
        "          conv_layers.append(child)\n",
        "\n",
        "plt.figure(figsize=(16, 16))\n",
        "for num_layer in range(len(model_weights)):\n",
        "  for i, filter in enumerate(model_weights[num_layer]):\n",
        "    if i == 16 or filter.shape[-1] == 1:\n",
        "      break\n",
        "    plt.subplot(16, 16, i+1)\n",
        "    plt.imshow(filter[0, :, :].detach(), cmap='gray')\n",
        "    plt.axis('off')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "LH8PhGIhoTJ7",
        "outputId": "97be3d55-46eb-4690-e931-4f7eb380468b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-3b78978b256d>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnum_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel_children\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_children\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'classification' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for weight, conv in zip(model_weights, conv_layers):\n",
        "    print(f\"CONV: {conv} ====> SHAPE: {weight.shape}\")"
      ],
      "metadata": {
        "id": "fTqb6_872Gew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = cv2.imread(f\"{DIR_TEST}/0.jpg\")\n",
        "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "plt.figure(figsize=(4, 4))\n",
        "plt.imshow(img)\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "transform = transforms.Compose([\n",
        " transforms.ToPILImage(),\n",
        " transforms.Resize((512, 512)),\n",
        " transforms.ToTensor(),\n",
        "])\n",
        "img = np.array(img)\n",
        "img = transform(img)\n",
        "img = img.unsqueeze(0)\n",
        "\n",
        "results = [conv_layers[0](img)]\n",
        "for i in range(1, len(conv_layers)):\n",
        "  results.append(conv_layers[i](results[-1]))\n",
        "outputs = results\n",
        "\n",
        "for num_layer in range(len(outputs)):\n",
        "  plt.figure(figsize=(15, 10))\n",
        "  layer_viz = outputs[num_layer][0, :, :, :]\n",
        "  layer_viz = layer_viz.data\n",
        "  for i, filter in enumerate(layer_viz):\n",
        "    if i == 64:\n",
        "      break\n",
        "    plt.subplot(7, 10, i + 1)\n",
        "    plt.imshow(filter, cmap='gray')\n",
        "    plt.axis(\"off\")\n",
        "  plt.show()\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "Ixg6Vz6UaV6a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}