{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Gzuv2run9Yxa"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import gc\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "#import numpy as\n",
        "import os\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from IPython.display import clear_output\n",
        "from PIL import Image\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets, models, transforms\n",
        "#from torchvision.models import resnet18\n",
        "from tqdm import tqdm\n",
        "import warnings"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DIR_TRAIN = \"/content/Classification-of-construction-equipment-objects/train/\"\n",
        "DIR_TEST = \"/content/Classification-of-construction-equipment-objects/test/\"\n",
        "\n",
        "PATH_TRAIN = DIR_TRAIN + \"train.csv\"\n",
        "PATH_TEST = DIR_TEST + \"test.csv\""
      ],
      "metadata": {
        "id": "JtcHBYJ9oxVx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://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@github.com/Sergey2110/Classification-of-construction-equipment-objects.git\n",
        "%cd Classification-of-construction-equipment-objects"
      ],
      "metadata": {
        "id": "puFPw94J3f_o",
        "outputId": "9efbc10b-6b55-4b86-ecdb-5ff7f86801da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Classification-of-construction-equipment-objects'...\n",
            "remote: Enumerating objects: 7203, done.\u001b[K\n",
            "remote: Counting objects: 100% (7203/7203), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7189/7189), done.\u001b[K\n",
            "remote: Total 7203 (delta 29), reused 7172 (delta 10), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (7203/7203), 27.08 MiB | 22.99 MiB/s, done.\n",
            "Resolving deltas: 100% (29/29), done.\n",
            "/content/Classification-of-construction-equipment-objects\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "warnings.simplefilter('ignore')"
      ],
      "metadata": {
        "id": "8LotOtK-sFsV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, data_df, transform=None):\n",
        "        self.data_df = data_df\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # достаем имя изображения и ее лейбл\n",
        "        image_name, label = self.data_df.iloc[idx]['ID_img'], self.data_df.iloc[idx]['class']\n",
        "\n",
        "        # читаем картинку. read the image\n",
        "        image = cv2.imread(DIR_TRAIN + f\"{image_name}\")\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        image = Image.fromarray(image)\n",
        "\n",
        "        # преобразуем, если нужно. transform it, if necessary\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, torch.tensor(label).long()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_df)"
      ],
      "metadata": {
        "id": "h72XzlyfMZek"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TestImageDataset(Dataset):\n",
        "    def __init__(self, data_df, transform=None):\n",
        "        self.data_df = data_df\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_name = self.data_df.iloc[idx]['ID_img']\n",
        "\n",
        "        # читаем картинку\n",
        "        image = cv2.imread(DIR_TEST + f\"{image_name}\")\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        image = Image.fromarray(image)\n",
        "\n",
        "        # преобразуем, если нужно\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_df)"
      ],
      "metadata": {
        "id": "7URcg7KlMqbV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Classification:\n",
        "    def __init__(self):\n",
        "        print(\"Обучающей выборки \", len(os.listdir(DIR_TRAIN)))\n",
        "        print(\"Тестовой выборки \", len(os.listdir(DIR_TEST)))\n",
        "\n",
        "        gc.collect()\n",
        "        # задаем преобразование изображения.\n",
        "        self.train_transform = transforms.Compose([\n",
        "            # transforms.Resize(256),\n",
        "            transforms.RandomResizedCrop(256),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225]),\n",
        "        ])\n",
        "\n",
        "        self.valid_transform = transforms.Compose([\n",
        "            # transforms.Resize(256),\n",
        "            transforms.RandomResizedCrop(256),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225]),\n",
        "        ])\n",
        "\n",
        "        self.data_df = pd.read_csv(PATH_TRAIN)\n",
        "        self.data_df = self.data_df[:10]\n",
        "        # self.data_df.head(3)\n",
        "\n",
        "        # разделим датасет на трейн и валидацию, чтобы смотреть на качество\n",
        "        self.train_df, self.valid_df = train_test_split(self.data_df, test_size=0.2, random_state=43)\n",
        "        train_dataset = ImageDataset(self.train_df, self.train_transform)\n",
        "        valid_dataset = ImageDataset(self.valid_df, self.valid_transform)\n",
        "\n",
        "        torch.manual_seed(43)\n",
        "\n",
        "        self.train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                                        batch_size=1,\n",
        "                                                        # shuffle=True,\n",
        "                                                        pin_memory=True,\n",
        "                                                        num_workers=2)\n",
        "\n",
        "        self.valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset,\n",
        "                                                        batch_size=1,\n",
        "                                                        # shuffle=True,\n",
        "                                                        pin_memory=True,\n",
        "                                                        num_workers=2)\n",
        "\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        self.dict_acc_for_batch = {\"train\": {}, \"test\": {}}\n",
        "        self.dict_loss_for_batch = {\"train\": {}, \"test\": {}}\n",
        "\n",
        "    def crossvalid(self, res_model=None, criterion=None, optimizer=None, dataset=None, k_fold=5):\n",
        "        train_score = pd.Series()\n",
        "        val_score = pd.Series()\n",
        "\n",
        "        total_size = len(dataset)\n",
        "        fraction = 1 / k_fold\n",
        "        seg = int(total_size * fraction)\n",
        "        # tr:train,val:valid; r:right,l:left;  eg: trrr: right index of right side train subset\n",
        "        # index: [trll,trlr],[vall,valr],[trrl,trrr]\n",
        "        for i in range(k_fold):\n",
        "            trll = 0\n",
        "            trlr = i * seg\n",
        "            vall = trlr\n",
        "            valr = i * seg + seg\n",
        "            trrl = valr\n",
        "            trrr = total_size\n",
        "\n",
        "            train_left_indices = list(range(trll, trlr))\n",
        "            train_right_indices = list(range(trrl, trrr))\n",
        "\n",
        "            train_indices = train_left_indices + train_right_indices\n",
        "            val_indices = list(range(vall, valr))\n",
        "\n",
        "            train_set = torch.utils.data.dataset.Subset(dataset, train_indices)\n",
        "            val_set = torch.utils.data.dataset.Subset(dataset, val_indices)\n",
        "\n",
        "            train_loader = torch.utils.data.DataLoader(train_set, batch_size=50,\n",
        "                                                       shuffle=True, num_workers=4)\n",
        "            val_loader = torch.utils.data.DataLoader(val_set, batch_size=50,\n",
        "                                                     shuffle=True, num_workers=4)\n",
        "            train_acc = self.train(res_model, criterion, optimizer, train_loader, val_loader, 1)\n",
        "            train_score.at[i] = train_acc\n",
        "            # val_acc = valid(res_model, criterion, optimizer, val_loader)\n",
        "            # val_score.at[i] = val_acc\n",
        "\n",
        "        return train_score, val_score\n",
        "\n",
        "    def plot_history(self, train_history, val_history, title = 'loss'):\n",
        "        plt.figure()\n",
        "        plt.title('{}'.format(title))\n",
        "\n",
        "        plt.plot(train_history, label='train', zorder=1)\n",
        "        plt.plot(val_history, label='val', zorder=1)\n",
        "\n",
        "        plt.legend(loc='best')\n",
        "        plt.xlabel('steps')\n",
        "        plt.grid()\n",
        "        plt.show()\n",
        "\n",
        "    def train(self, criterion, optimizer, train_dataloader, test_dataloader, NUM_EPOCH=15, show_img=False):\n",
        "        train_loss_log = []\n",
        "        val_loss_log = []\n",
        "        train_acc_log = []\n",
        "        val_acc_log = []\n",
        "\n",
        "        train_loss_log_for_batch = []\n",
        "        val_loss_log_for_batch = []\n",
        "        train_acc_log_for_batch = []\n",
        "        val_acc_log_for_batch = []\n",
        "\n",
        "        for epoch in tqdm(range(NUM_EPOCH)):\n",
        "            self.model.train()\n",
        "            train_loss = 0.\n",
        "            train_size = 0\n",
        "            train_pred = 0.\n",
        "\n",
        "            for imgs, labels in train_dataloader:\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                imgs = imgs.to(self.device)\n",
        "                labels = labels.to(self.device)\n",
        "\n",
        "                y_pred = self.model(imgs)\n",
        "\n",
        "                loss = criterion(y_pred, labels)\n",
        "                loss.backward()\n",
        "\n",
        "                train_size += y_pred.size(0)\n",
        "                train_loss += loss.item()\n",
        "                train_loss_log_for_batch.append(loss.data / y_pred.size(0))\n",
        "                train_pred += (y_pred.argmax(1) == labels).sum()\n",
        "                train_acc_log_for_batch.append(train_pred / y_pred.size(0))\n",
        "                optimizer.step()\n",
        "\n",
        "            train_loss_log.append(train_loss / train_size)\n",
        "            train_acc_log.append(train_pred.item() / train_size)\n",
        "\n",
        "            self.dict_loss_for_batch[\"train\"].update({epoch: train_loss_log_for_batch[:]})\n",
        "            self.dict_acc_for_batch[\"train\"].update({epoch: train_acc_log_for_batch[:]})\n",
        "\n",
        "            # if show_img and epoch > (epoch - 2) and train_pred / train_size < 0.9:\n",
        "            #     for j in range(4):\n",
        "            #         show_input(imgs[j].cpu(),\n",
        "            #                    title=f\"{labels[j]} {list_file[list_index_val[j + i * batch_size_v]][0]}\")\n",
        "            #         print(f\" epoch = {epoch} acc = {(train_pred / train_size) / batch_size_v}\")\n",
        "\n",
        "            val_loss = 0.\n",
        "            val_size = 0\n",
        "            val_pred = 0.\n",
        "            self.model.eval()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for imgs, labels in test_dataloader:\n",
        "                    imgs = imgs.to(self.device)\n",
        "                    labels = labels.to(self.device)\n",
        "\n",
        "                    pred = self.model(imgs)\n",
        "                    loss = criterion(pred, labels)\n",
        "\n",
        "                    val_size += pred.size(0)\n",
        "                    val_loss += loss.item()\n",
        "                    val_loss_log_for_batch.append(loss.data / pred.size(0))\n",
        "                    val_pred += (pred.argmax(1) == labels).sum()\n",
        "                    val_acc_log_for_batch.append(val_pred / pred.size(0))\n",
        "\n",
        "            val_loss_log.append(val_loss / val_size)\n",
        "            val_acc_log.append(val_pred.item() / val_size)\n",
        "\n",
        "            self.dict_loss_for_batch[\"test\"].update({epoch: val_loss_log_for_batch[:]})\n",
        "            self.dict_acc_for_batch[\"test\"].update({epoch: val_acc_log_for_batch[:]})\n",
        "\n",
        "        # clear_output()\n",
        "        # self.plot_history(train_loss_log, val_loss_log, 'loss')\n",
        "\n",
        "        # print('Train loss:', (train_loss / train_size) * 100)\n",
        "        # print('Val loss:', (val_loss / val_size) * 100)\n",
        "        # print('Train acc:', (train_pred.item() / train_size) * 100)\n",
        "        # print('Val acc:', (val_pred.item() / val_size) * 100)\n",
        "\n",
        "        return train_loss_log, train_acc_log, val_loss_log, val_acc_log\n",
        "\n",
        "    def watch_img(self):\n",
        "        # посмотрим на картинки. Не забудем указать корретный путь до папки\n",
        "        sns.countplot(x=\"class\", data=self.data_df)\n",
        "        fig, axs = plt.subplots(2, 4, figsize=(16, 8))\n",
        "        fig.suptitle(f'Автомобиль {\" \" * 105} Кран', fontsize=14)\n",
        "\n",
        "        for i, name in zip(range(4), self.data_df[self.data_df['class'] == 1].sample(4, random_state=42)['ID_img']):\n",
        "            img = plt.imread(DIR_TRAIN + f\"{name}\")\n",
        "            axs[i // 2, (i % 2)].imshow(img)\n",
        "            axs[i // 2, (i % 2)].axis('off')\n",
        "\n",
        "        for i, name in zip(range(4), self.data_df[self.data_df['class'] == 0].sample(4, random_state=42)['ID_img']):\n",
        "            img = plt.imread(DIR_TRAIN + f\"{name}\")\n",
        "            axs[i // 2, (i % 2) + 2].imshow(img)\n",
        "            axs[i // 2, (i % 2) + 2].axis('off')\n",
        "\n",
        "        fig.tight_layout()\n",
        "        fig.subplots_adjust(top=0.88)\n",
        "\n",
        "    def train_model(self):\n",
        "        self.model = models.resnet152(pretrained=True)\n",
        "        self.model.fc = nn.Linear(2048, 8)\n",
        "        self.model = self.model.to(self.device)\n",
        "        criterion = torch.nn.CrossEntropyLoss()\n",
        "        optimizer = torch.optim.Adam(self.model.fc.parameters(), lr=0.01)\n",
        "        train_loss_log, train_acc_log, val_loss_log, val_acc_log = self.train(criterion,\n",
        "                                                                              optimizer,\n",
        "                                                                              self.train_loader,\n",
        "                                                                              self.valid_loader,\n",
        "                                                                              1)\n",
        "        return train_loss_log, train_acc_log, val_loss_log, val_acc_log\n",
        "\n",
        "    def evaluation_model(self):\n",
        "        valid_predicts = []\n",
        "        self.model.eval()\n",
        "        for imgs, _ in tqdm(self.valid_loader):\n",
        "            imgs = imgs.to(self.device)\n",
        "            pred = self.model(imgs)\n",
        "            pred_numpy = pred.cpu().detach().numpy()\n",
        "            for class_obj in pred_numpy:\n",
        "                index, max_value = max(enumerate(class_obj), key=lambda i_v: i_v[1])\n",
        "                valid_predicts.append(index)\n",
        "        self.valid_df[\"pred\"] = valid_predicts\n",
        "        val_accuracy = recall_score(self.valid_df['class'].values, self.valid_df['pred'].values, average=\"macro\")\n",
        "        print(f\"Validation accuracy = {val_accuracy}\")\n",
        "\n",
        "        self.test_df = pd.read_csv(PATH_TEST)\n",
        "        self.test_df = self.test_df.drop([\"class\"], axis=1)\n",
        "        self.test_dataset = TestImageDataset(self.test_df, self.valid_transform)\n",
        "        self.test_loader = torch.utils.data.DataLoader(dataset=self.test_dataset,\n",
        "                                                       batch_size=32,\n",
        "                                                       # shuffle=True,\n",
        "                                                       pin_memory=True,\n",
        "                                                       num_workers=2)\n",
        "\n",
        "    def create_submit(self):\n",
        "        self.model.eval()\n",
        "        predicts = []\n",
        "        for imgs in tqdm(self.test_loader):\n",
        "            imgs = imgs.to(self.device)\n",
        "            pred = self.model(imgs)\n",
        "            for class_obj in pred:\n",
        "                index, max_value = max(enumerate(class_obj), key=lambda i_v: i_v[1])\n",
        "                predicts.append(index)\n",
        "\n",
        "        self.test_df[\"class\"] = predicts\n",
        "        self.test_df.head()\n",
        "        self.test_df.to_csv(\"submit.csv\", index=False)"
      ],
      "metadata": {
        "id": "WTzOYXnLr615"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classification = Classification()\n",
        "print(list(classification.train_loader))\n",
        "# acc_loss = classification.train_model()\n",
        "# classification.evaluation_model()\n",
        "# classification.create_submit()"
      ],
      "metadata": {
        "id": "NtXeREGor-ln",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35a73ffa-aa81-4bec-af0e-88cf67b5aff6"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Обучающей выборки  4993\n",
            "Тестовой выборки  2141\n",
            "[[tensor([[[[ 2.1975,  2.1975,  2.2147,  ...,  0.8618,  0.8789,  0.8789],\n",
            "          [ 2.1975,  2.1975,  2.2147,  ...,  0.8618,  0.8789,  0.8789],\n",
            "          [ 2.1975,  2.1975,  2.2147,  ...,  0.8789,  0.8961,  0.8961],\n",
            "          ...,\n",
            "          [-0.2513, -0.2513, -0.2171,  ..., -0.4568, -0.4911, -0.4911],\n",
            "          [-0.3027, -0.3027, -0.2684,  ..., -0.6281, -0.6452, -0.6452],\n",
            "          [-0.3027, -0.3027, -0.2684,  ..., -0.6281, -0.6452, -0.6452]],\n",
            "\n",
            "         [[ 2.3761,  2.3761,  2.3936,  ...,  0.7829,  0.8004,  0.8004],\n",
            "          [ 2.3761,  2.3761,  2.3936,  ...,  0.7829,  0.8004,  0.8004],\n",
            "          [ 2.3761,  2.3761,  2.3936,  ...,  0.8004,  0.8179,  0.8179],\n",
            "          ...,\n",
            "          [-0.0574, -0.0574, -0.0224,  ..., -0.2325, -0.2675, -0.2675],\n",
            "          [-0.1099, -0.1099, -0.0749,  ..., -0.4076, -0.4251, -0.4251],\n",
            "          [-0.1099, -0.1099, -0.0749,  ..., -0.4076, -0.4251, -0.4251]],\n",
            "\n",
            "         [[ 2.5877,  2.5877,  2.6051,  ...,  0.8448,  0.8622,  0.8622],\n",
            "          [ 2.5877,  2.5877,  2.6051,  ...,  0.8448,  0.8622,  0.8622],\n",
            "          [ 2.5877,  2.5877,  2.6051,  ...,  0.8622,  0.8797,  0.8797],\n",
            "          ...,\n",
            "          [ 0.2173,  0.2173,  0.2522,  ..., -0.0441, -0.0790, -0.0790],\n",
            "          [ 0.1651,  0.1651,  0.1999,  ..., -0.2184, -0.2358, -0.2358],\n",
            "          [ 0.1651,  0.1651,  0.1999,  ..., -0.2184, -0.2358, -0.2358]]]]), tensor([4])], [tensor([[[[-1.1247, -1.1247, -1.1075,  ..., -1.2445, -1.2445, -1.2445],\n",
            "          [-1.1247, -1.1247, -1.1075,  ..., -1.2445, -1.2445, -1.2445],\n",
            "          [-0.9877, -0.9877, -0.9705,  ..., -1.2445, -1.2445, -1.2445],\n",
            "          ...,\n",
            "          [ 1.6667,  1.6667,  1.6667,  ..., -1.5699, -1.5699, -1.5699],\n",
            "          [ 1.6667,  1.6667,  1.6667,  ..., -1.5528, -1.5528, -1.5528],\n",
            "          [ 1.6667,  1.6667,  1.6667,  ..., -1.5528, -1.5528, -1.5528]],\n",
            "\n",
            "         [[-0.9503, -0.9503, -0.9328,  ..., -1.1078, -1.1078, -1.1078],\n",
            "          [-0.9503, -0.9503, -0.9328,  ..., -1.1078, -1.1078, -1.1078],\n",
            "          [-0.8102, -0.8102, -0.7927,  ..., -1.1078, -1.1078, -1.1078],\n",
            "          ...,\n",
            "          [ 1.2556,  1.2556,  1.2556,  ..., -1.5630, -1.5630, -1.5630],\n",
            "          [ 1.2556,  1.2556,  1.2556,  ..., -1.5455, -1.5455, -1.5455],\n",
            "          [ 1.2556,  1.2556,  1.2556,  ..., -1.5455, -1.5455, -1.5455]],\n",
            "\n",
            "         [[-0.7413, -0.7413, -0.7238,  ..., -1.1073, -1.1073, -1.1073],\n",
            "          [-0.7413, -0.7413, -0.7238,  ..., -1.1073, -1.1073, -1.1073],\n",
            "          [-0.5844, -0.5844, -0.5670,  ..., -1.1073, -1.1073, -1.1073],\n",
            "          ...,\n",
            "          [ 1.3851,  1.3851,  1.3851,  ..., -1.2641, -1.2641, -1.2641],\n",
            "          [ 1.3851,  1.3851,  1.3851,  ..., -1.2467, -1.2467, -1.2467],\n",
            "          [ 1.3851,  1.3851,  1.3851,  ..., -1.2467, -1.2467, -1.2467]]]]), tensor([1])], [tensor([[[[-1.1247, -1.1075, -1.0048,  ..., -1.8268, -1.7754, -1.7754],\n",
            "          [-1.1247, -1.1075, -1.0219,  ..., -1.8268, -1.7754, -1.7754],\n",
            "          [-1.1760, -1.1589, -1.0733,  ..., -1.8268, -1.8097, -1.8097],\n",
            "          ...,\n",
            "          [ 1.7523,  1.7523,  1.8208,  ..., -0.9192, -0.8678, -0.8507],\n",
            "          [ 1.8037,  1.8037,  1.8722,  ..., -0.8678, -0.7822, -0.7650],\n",
            "          [ 1.8208,  1.8208,  1.8722,  ..., -0.8507, -0.7650, -0.7479]],\n",
            "\n",
            "         [[-1.0728, -1.0553, -0.9503,  ..., -1.8256, -1.7906, -1.7906],\n",
            "          [-1.0728, -1.0553, -0.9678,  ..., -1.8256, -1.7906, -1.7906],\n",
            "          [-1.1253, -1.1078, -1.0378,  ..., -1.8081, -1.8081, -1.8081],\n",
            "          ...,\n",
            "          [ 1.5182,  1.5357,  1.6057,  ..., -0.7577, -0.7052, -0.6877],\n",
            "          [ 1.5882,  1.5882,  1.6583,  ..., -0.6877, -0.6001, -0.5826],\n",
            "          [ 1.6057,  1.6057,  1.6583,  ..., -0.6702, -0.5826, -0.5651]],\n",
            "\n",
            "         [[-0.9330, -0.9156, -0.8284,  ..., -1.5779, -1.5256, -1.5256],\n",
            "          [-0.9330, -0.9156, -0.8458,  ..., -1.5779, -1.5256, -1.5256],\n",
            "          [-0.9853, -0.9678, -0.8981,  ..., -1.5604, -1.5256, -1.5256],\n",
            "          ...,\n",
            "          [-0.7761, -0.7936, -0.8284,  ..., -0.5147, -0.4275, -0.4275],\n",
            "          [-0.7238, -0.7413, -0.8110,  ..., -0.4275, -0.3230, -0.3055],\n",
            "          [-0.7238, -0.7413, -0.8110,  ..., -0.4101, -0.3055, -0.2881]]]]), tensor([3])], [tensor([[[[-0.5767, -0.5767, -0.5767,  ..., -0.4911, -0.4911, -0.4911],\n",
            "          [-0.5767, -0.5767, -0.5767,  ..., -0.4911, -0.4911, -0.4911],\n",
            "          [-0.5767, -0.5767, -0.5767,  ..., -0.4911, -0.4911, -0.4911],\n",
            "          ...,\n",
            "          [-1.2103, -1.2103, -1.2103,  ..., -1.2788, -1.2788, -1.2788],\n",
            "          [-1.2103, -1.2103, -1.2103,  ..., -1.2788, -1.2788, -1.2788],\n",
            "          [-1.2103, -1.2103, -1.2103,  ..., -1.2788, -1.2788, -1.2788]],\n",
            "\n",
            "         [[-0.4251, -0.4251, -0.4251,  ..., -0.4251, -0.4251, -0.4251],\n",
            "          [-0.4251, -0.4251, -0.4251,  ..., -0.4251, -0.4251, -0.4251],\n",
            "          [-0.4251, -0.4251, -0.4251,  ..., -0.4251, -0.4251, -0.4251],\n",
            "          ...,\n",
            "          [-1.2304, -1.2304, -1.2304,  ..., -1.1604, -1.1604, -1.1604],\n",
            "          [-1.2304, -1.2304, -1.2304,  ..., -1.1604, -1.1604, -1.1604],\n",
            "          [-1.2304, -1.2304, -1.2304,  ..., -1.1604, -1.1604, -1.1604]],\n",
            "\n",
            "         [[-0.2881, -0.2881, -0.2881,  ..., -0.4624, -0.4624, -0.4624],\n",
            "          [-0.2881, -0.2881, -0.2881,  ..., -0.4624, -0.4624, -0.4624],\n",
            "          [-0.2881, -0.2881, -0.2881,  ..., -0.4624, -0.4624, -0.4624],\n",
            "          ...,\n",
            "          [-1.1770, -1.1770, -1.1770,  ..., -1.0201, -1.0201, -1.0201],\n",
            "          [-1.1770, -1.1770, -1.1770,  ..., -1.0201, -1.0201, -1.0201],\n",
            "          [-1.1770, -1.1770, -1.1770,  ..., -1.0201, -1.0201, -1.0201]]]]), tensor([4])], [tensor([[[[ 1.3927,  1.3927,  1.3927,  ...,  0.6906,  0.6734,  0.6734],\n",
            "          [ 1.3927,  1.3927,  1.3927,  ...,  0.6906,  0.6734,  0.6734],\n",
            "          [ 1.3927,  1.3927,  1.3927,  ...,  0.6906,  0.6734,  0.6734],\n",
            "          ...,\n",
            "          [ 0.0741,  0.0741,  0.0227,  ..., -1.6898, -1.6898, -1.6898],\n",
            "          [-0.1143, -0.1143, -0.1486,  ..., -1.6898, -1.6898, -1.6898],\n",
            "          [-0.1143, -0.1143, -0.1486,  ..., -1.6898, -1.6898, -1.6898]],\n",
            "\n",
            "         [[ 1.7633,  1.7633,  1.7633,  ...,  1.1155,  1.0980,  1.0980],\n",
            "          [ 1.7633,  1.7633,  1.7633,  ...,  1.1155,  1.0980,  1.0980],\n",
            "          [ 1.7633,  1.7633,  1.7633,  ...,  1.1155,  1.0980,  1.0980],\n",
            "          ...,\n",
            "          [-0.2675, -0.3025, -0.3901,  ..., -1.6155, -1.6155, -1.6155],\n",
            "          [-0.4601, -0.4776, -0.5476,  ..., -1.6155, -1.6155, -1.6155],\n",
            "          [-0.4601, -0.4776, -0.5476,  ..., -1.6155, -1.6155, -1.6155]],\n",
            "\n",
            "         [[ 2.2217,  2.2217,  2.2217,  ...,  1.7337,  1.7163,  1.7163],\n",
            "          [ 2.2217,  2.2217,  2.2217,  ...,  1.7337,  1.7163,  1.7163],\n",
            "          [ 2.2217,  2.2217,  2.2217,  ...,  1.7337,  1.7163,  1.7163],\n",
            "          ...,\n",
            "          [-0.5147, -0.5321, -0.6193,  ..., -1.4559, -1.4559, -1.4559],\n",
            "          [-0.7064, -0.7238, -0.7761,  ..., -1.4559, -1.4559, -1.4559],\n",
            "          [-0.7064, -0.7238, -0.7761,  ..., -1.4559, -1.4559, -1.4559]]]]), tensor([6])], [tensor([[[[2.2318, 2.2318, 2.2318,  ..., 2.2489, 2.2489, 2.2489],\n",
            "          [2.2318, 2.2318, 2.2318,  ..., 2.2489, 2.2489, 2.2489],\n",
            "          [2.2318, 2.2318, 2.2318,  ..., 2.2489, 2.2489, 2.2489],\n",
            "          ...,\n",
            "          [2.2318, 2.2318, 2.2318,  ..., 0.6563, 0.6563, 0.6563],\n",
            "          [2.2318, 2.2318, 2.2318,  ..., 0.6563, 0.6563, 0.6563],\n",
            "          [2.2318, 2.2318, 2.2318,  ..., 0.6563, 0.6563, 0.6563]],\n",
            "\n",
            "         [[2.4111, 2.4111, 2.4111,  ..., 2.3936, 2.3936, 2.3936],\n",
            "          [2.4111, 2.4111, 2.4111,  ..., 2.3936, 2.3936, 2.3936],\n",
            "          [2.4111, 2.4111, 2.4111,  ..., 2.3936, 2.3936, 2.3936],\n",
            "          ...,\n",
            "          [2.4111, 2.4111, 2.4111,  ..., 0.8004, 0.8004, 0.8004],\n",
            "          [2.4111, 2.4111, 2.4111,  ..., 0.8004, 0.8004, 0.8004],\n",
            "          [2.4111, 2.4111, 2.4111,  ..., 0.8004, 0.8004, 0.8004]],\n",
            "\n",
            "         [[2.6226, 2.6226, 2.6226,  ..., 2.6400, 2.6400, 2.6400],\n",
            "          [2.6226, 2.6226, 2.6226,  ..., 2.6400, 2.6400, 2.6400],\n",
            "          [2.6226, 2.6226, 2.6226,  ..., 2.6400, 2.6400, 2.6400],\n",
            "          ...,\n",
            "          [2.6226, 2.6226, 2.6226,  ..., 0.9842, 0.9842, 0.9842],\n",
            "          [2.6226, 2.6226, 2.6226,  ..., 0.9842, 0.9842, 0.9842],\n",
            "          [2.6226, 2.6226, 2.6226,  ..., 0.9842, 0.9842, 0.9842]]]]), tensor([4])], [tensor([[[[ 0.9817,  0.9817,  0.9817,  ...,  1.5639,  1.5639,  1.5639],\n",
            "          [ 0.9817,  0.9817,  0.9817,  ...,  1.5639,  1.5639,  1.5639],\n",
            "          [ 0.9646,  0.9646,  0.9646,  ...,  1.5639,  1.5639,  1.5639],\n",
            "          ...,\n",
            "          [-0.9020, -0.9020, -0.9192,  ..., -0.2513, -0.2684, -0.2684],\n",
            "          [-0.9020, -0.9020, -0.9192,  ..., -0.2684, -0.2856, -0.2856],\n",
            "          [-0.9020, -0.9020, -0.9192,  ..., -0.2684, -0.2856, -0.2856]],\n",
            "\n",
            "         [[-1.0203, -1.0203, -1.0203,  ...,  1.9384,  1.9384,  1.9384],\n",
            "          [-1.0203, -1.0203, -1.0203,  ...,  1.9384,  1.9384,  1.9384],\n",
            "          [-1.0553, -1.0553, -1.0553,  ...,  1.9384,  1.9384,  1.9384],\n",
            "          ...,\n",
            "          [-1.1078, -1.1078, -1.1078,  ..., -0.2500, -0.2675, -0.2675],\n",
            "          [-1.1078, -1.1078, -1.1078,  ..., -0.2675, -0.2850, -0.2850],\n",
            "          [-1.1078, -1.1078, -1.1078,  ..., -0.2675, -0.2850, -0.2850]],\n",
            "\n",
            "         [[-1.0724, -1.0724, -1.0724,  ...,  2.1171,  2.1171,  2.1171],\n",
            "          [-1.0724, -1.0724, -1.0724,  ...,  2.1171,  2.1171,  2.1171],\n",
            "          [-1.1247, -1.1247, -1.1247,  ...,  2.1171,  2.1171,  2.1171],\n",
            "          ...,\n",
            "          [-0.9504, -0.9504, -0.9678,  ..., -0.7587, -0.7761, -0.7761],\n",
            "          [-0.9504, -0.9504, -0.9678,  ..., -0.7761, -0.7936, -0.7936],\n",
            "          [-0.9504, -0.9504, -0.9678,  ..., -0.7761, -0.7936, -0.7936]]]]), tensor([4])], [tensor([[[[-1.4500, -1.4500, -1.0390,  ...,  1.7523,  1.7180,  1.7180],\n",
            "          [-1.4329, -1.4329, -1.0904,  ...,  1.7352,  1.7009,  1.7009],\n",
            "          [-1.3473, -1.3473, -1.2274,  ...,  1.7009,  1.6667,  1.6667],\n",
            "          ...,\n",
            "          [-1.0562, -1.0562, -1.2445,  ..., -0.7308, -0.6109, -0.6109],\n",
            "          [-1.1418, -1.1418, -1.2959,  ..., -0.3369, -0.2171, -0.2171],\n",
            "          [-1.1589, -1.1589, -1.3130,  ..., -0.2171, -0.0972, -0.0972]],\n",
            "\n",
            "         [[-1.5630, -1.5630, -1.1253,  ...,  0.9755,  0.9230,  0.9230],\n",
            "          [-1.5280, -1.5280, -1.1604,  ...,  0.9580,  0.9055,  0.9055],\n",
            "          [-1.4405, -1.4405, -1.2829,  ...,  0.9230,  0.8704,  0.8704],\n",
            "          ...,\n",
            "          [-0.9678, -0.9678, -1.1604,  ..., -0.6702, -0.5476, -0.5476],\n",
            "          [-1.0553, -1.0553, -1.2129,  ..., -0.2675, -0.1450, -0.1450],\n",
            "          [-1.0728, -1.0728, -1.2304,  ..., -0.1450, -0.0224, -0.0224]],\n",
            "\n",
            "         [[-1.3339, -1.3339, -0.8981,  ..., -0.8110, -0.8981, -0.8981],\n",
            "          [-1.2990, -1.2990, -0.9330,  ..., -0.8458, -0.9330, -0.9330],\n",
            "          [-1.1770, -1.1770, -1.0201,  ..., -0.9330, -1.0201, -1.0201],\n",
            "          ...,\n",
            "          [-0.7761, -0.7761, -0.9678,  ..., -0.5321, -0.4101, -0.4101],\n",
            "          [-0.8633, -0.8633, -1.0201,  ..., -0.1312, -0.0092, -0.0092],\n",
            "          [-0.8807, -0.8807, -1.0376,  ..., -0.0092,  0.1128,  0.1128]]]]), tensor([3])]]\n"
          ]
        }
      ]
    }
  ]
}